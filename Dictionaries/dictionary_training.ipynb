{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary Training\n",
    "\n",
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn import linear_model\n",
    "from scipy import signal\n",
    "from scipy.signal import convolve2d\n",
    "import cvxpy as cp\n",
    "import time\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Patch from Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hIm: high resolution image\n",
    "#patch_size: patch size that we want to retrieve from image\n",
    "#num_patches: number of patches we want to sample\n",
    "#upscale: upscale factor from low resolution image to high resolution image\n",
    "def sample_patches(hIm, patch_size, num_patches, upscale):\n",
    "    #Convert RBG to Grayscale Images\n",
    "    if hIm.shape[2] == 3:\n",
    "        hIm = cv2.cvtColor(hIm, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "    #Blur the High Resolution Image a bit\n",
    "    blur_kernel = np.ones(shape = (3, 3)) / 9\n",
    "    blurred_hIm = convolve2d(hIm, blur_kernel, mode = 'same')\n",
    "        \n",
    "    #Generate Low Resolution Image\n",
    "    lIm = cv2.resize(blurred_hIm, tuple(int(x * (1/upscale)) for x in blurred_hIm.shape)[::-1], interpolation = cv2.INTER_NEAREST)\n",
    "    lIm = cv2.resize(lIm, blurred_hIm.shape[::-1], interpolation = cv2.INTER_NEAREST)\n",
    "    \n",
    "    #Get dimensions of High Resolution Image\n",
    "    nrow, ncol = hIm.shape\n",
    "    \n",
    "    #Get posible values of (x, y) that is top left corner of patch. The (x,y) coordinates are in the coordinate space of the High Resolution Image\n",
    "    x = np.random.permutation(np.arange(0, nrow - 2 * patch_size - 1)) + patch_size\n",
    "    y = np.random.permutation(np.arange(0, ncol - 2 * patch_size - 1)) + patch_size\n",
    "    \n",
    "    #Generated Meshgrid\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    #Flatten X and Y column wise\n",
    "    xrow = X.flatten(order = 'F')\n",
    "    ycol = Y.flatten(order = 'F')\n",
    "    \n",
    "    #If we have less patches than potential (x, y) coordinates, we have to truncate the list of (x, y) coordinates\n",
    "    if num_patches < len(xrow):\n",
    "        xrow = xrow[: num_patches]\n",
    "        ycol = ycol[: num_patches]\n",
    "    \n",
    "    num_patches = len(xrow)\n",
    "    \n",
    "    #Store High and Low Resolution Patches\n",
    "    H = np.zeros(shape = (patch_size ** 2, num_patches))\n",
    "    L = np.zeros(shape = (4 * (patch_size ** 2), num_patches))\n",
    "    \n",
    "    #Compute first order derivatives\n",
    "    hf1 = np.array([-1,0,1]).reshape((1, -1))\n",
    "    vf1 = hf1.T\n",
    "    \n",
    "    lImG11 = signal.convolve2d(lIm, hf1[::-1, ::-1],'same') #row wise 1st order derivative\n",
    "    lImG12 = signal.convolve2d(lIm, vf1[::-1, ::-1],'same') #column wise 1st order derivative\n",
    "    \n",
    "    #Compute second order derivatives\n",
    "    hf2 = np.array([1,0,-2,0,1]).reshape((1, -1))\n",
    "    vf2 = hf2.T\n",
    "    \n",
    "    lImG21 = signal.convolve2d(lIm, hf2[::-1, ::-1], 'same') #row wise 2nd order derivative\n",
    "    lImG22 = signal.convolve2d(lIm, vf2[::-1, ::-1], 'same') #column wise 2nd order derivative\n",
    "    \n",
    "    #Extract Patches\n",
    "    for idx in range(num_patches):\n",
    "        row, col = xrow[idx], ycol[idx]\n",
    "        \n",
    "        #Get the patch from High Resolution Image\n",
    "        Hpatch = hIm[row: row + patch_size, col: col + patch_size].flatten(order = 'F')\n",
    "        H[:, idx] = Hpatch - np.mean(Hpatch) #Store High Resolution Patch\n",
    "        \n",
    "        #Get the patch from Low Resolution Image\n",
    "        Lpatch1 = lImG11[row:row+patch_size,col:col+patch_size].flatten(order = 'F')\n",
    "        Lpatch2 = lImG12[row:row+patch_size,col:col+patch_size].flatten(order = 'F')\n",
    "        Lpatch3 = lImG21[row:row+patch_size,col:col+patch_size].flatten(order = 'F')\n",
    "        Lpatch4 = lImG22[row:row+patch_size,col:col+patch_size].flatten(order = 'F')\n",
    "\n",
    "        Lpatch = np.concatenate((Lpatch1, Lpatch2, Lpatch3, Lpatch4))\n",
    "        L[:, idx] = Lpatch #Store Low Resolution Patch\n",
    "    \n",
    "    return H, L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly Sample Patches From Training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly Sample Patches\n",
    "#img_path: path of image\n",
    "#img_type: type of image\n",
    "#patch_size: size of patch\n",
    "#num_patch: number of patches we want to sample\n",
    "#upscale: upscale factor from low resolution image to high resolution image\n",
    "def rnd_smp_patch(img_path, img_type, patch_size, num_patch, upscale):\n",
    "    #image directory with training image path\n",
    "    img_dir = [file for file in os.listdir(img_path) if file.endswith(img_type)]\n",
    "        \n",
    "    Xh = [] #Store High Resolution Patches\n",
    "    Xl = [] #Store Low Resolution Patches\n",
    "    \n",
    "    img_num = len(img_dir) #Total number of images\n",
    "    nper_img = np.zeros(shape = (img_num, )) #number of patches per image\n",
    "    print(\"Number of Images From Training Dataset we have Sampled: \", img_num)\n",
    "    \n",
    "    #Store total size of all images\n",
    "    for idx in range(img_num):\n",
    "        im = cv2.imread(os.path.join(img_path, img_dir[idx]))\n",
    "        nper_img[idx] = im.size\n",
    "    \n",
    "    nper_img = np.floor(nper_img * num_patch / np.sum(nper_img)).astype(int) #number of patches per image\n",
    "    \n",
    "    #iterate through images\n",
    "    for idx in range(img_num):\n",
    "        patch_num = nper_img[idx] #number of patches from this image to select\n",
    "        im = cv2.imread(os.path.join(img_path, img_dir[idx])) #Get image\n",
    "        \n",
    "        #Sample the Patches\n",
    "        H, L = sample_patches(im, patch_size, patch_num, upscale)\n",
    "                \n",
    "        #Append to Xh and Xl\n",
    "        Xh.append(H)\n",
    "        Xl.append(L)\n",
    "    \n",
    "    Xh, Xl = np.concatenate(Xh, axis = 1), np.concatenate(Xl, axis = 1) #Concatenate Patches into numpy array\n",
    "    return Xh, Xl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jointly Train Dictionaries\n",
    "\n",
    "The goal of jointly training the low-resolution and high-resolution dictionaries is to ensure that the respective columns of each of the dictionaries correlate with each other. A low-resolution image represented by a linear combination of column vectors from the low-resolution dictionary should produce a similar, higher resolution image if the image is produced by the same linear combination of column vectors from the high-resolution dictionary. This allows us to generate high-resolution patches by first determining a sparse representation in terms of the low-resolution dictionary of low-resolution patches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jointly train dictionaries\n",
    "#Xh: High Resolution Patches\n",
    "#Xl: Low Resolution Patches\n",
    "#dict_size: size of dictionary\n",
    "#step size: step size for Quadratic Programming step\n",
    "#lamb: lambda for linear programming step\n",
    "#threshold: threshold for iterations in quadratic programming\n",
    "#max_iter: maximum number of iterations\n",
    "def train_coupled_dict(Xh, Xl, dict_size, step_size, lamb, threshold, max_iter):\n",
    "    print(\"STARTING TRAINING\")\n",
    "    \n",
    "    #Get shape of Patch Data\n",
    "    N, M = Xh.shape[0], Xl.shape[0]\n",
    "    \n",
    "    #Get Constants\n",
    "    a1 = 1 / np.sqrt(N)\n",
    "    a2 = 1 / np.sqrt(M)\n",
    "    \n",
    "    #Initialize Xc\n",
    "    Xc = np.concatenate((a1 * Xh, a2 * Xl), axis = 0)\n",
    "    print(f\"Xc shape: {Xc.shape}\")\n",
    "    \n",
    "    #Initialize D as a random Gaussian Matrix\n",
    "    Dc = np.random.normal(size = (N + M, dict_size))\n",
    "    Dc = normalize(Dc)\n",
    "    print(f\"Dc shape: {Dc.shape}\")\n",
    "    \n",
    "    #cap maximum iterations at max_iter\n",
    "    for iter in range(max_iter):\n",
    "        Z = lasso_optimization(Xc, Dc, lamb) #Run lasso Optimization\n",
    "        Xc_pred = Dc @ Z #Get XC_pred to compute loss metric\n",
    "        print(f\"Iteration {iter + 1}/{max_iter} Linear Programming Stat: {np.linalg.norm(Xc - Xc_pred) / np.linalg.norm(Xc)}\")\n",
    "        \n",
    "        Dc = quadratic_programming(Xc, Z, Dc.shape[0], Dc.shape[1], step_size, threshold, 30) #Run Quadratic Programming Step\n",
    "        Xc_pred = Dc @ Z #Get XC_pred to compute loss metric\n",
    "        print(f\"Iteration {iter + 1}/{max_iter} Quadratic Programming Stat: {np.linalg.norm(Xc - Xc_pred) / np.linalg.norm(Xc)}\")\n",
    "    \n",
    "    return Dc\n",
    "\n",
    "#Lasso Optimization\n",
    "#Goal: Solve for value of Z to minimize ||Xc - Dc Z||_2^2 + lamb * ||Z||_1\n",
    "def lasso_optimization(Xc, Dc, lamb):\n",
    "    clf = linear_model.Lasso(alpha = lamb, max_iter = 100000, fit_intercept = False)\n",
    "    clf.fit(Dc, Xc)\n",
    "    return clf.coef_.T\n",
    "        \n",
    "#prox operator given x and alpha\n",
    "def prox(x, alpha):\n",
    "    return np.piecewise(x, [x < -alpha, (x >= -alpha) & (x <= alpha), x >= alpha], [lambda x: x + alpha, 0, lambda x: x - alpha])\n",
    "\n",
    "## Solve the Linear Programming Portion of Joint Dictionary Training\n",
    "## Goal: Find Z that minimizes || X - DZ||_2^2 + lambda * ||Z||_1\n",
    "def linear_programming(X: np.ndarray, D: np.ndarray, Zr, Zc, step_size, lamb, threshold, max_iter):\n",
    "    Z = np.random.normal(size = (Zr, Zc))\n",
    "    \n",
    "    #Run Proximal Gradient Descent\n",
    "    loss = (np.linalg.norm(X - (D @ Z)) ** 2) + (lamb * np.sum(np.abs(Z)))\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        grad = (-2 * (D.T @ X)) + (2 * (D.T @ D @ Z))\n",
    "        \n",
    "        #Update Z\n",
    "        Z = Z - (step_size * grad)\n",
    "        Z = prox(Z, step_size * lamb)\n",
    "        \n",
    "        loss = (np.linalg.norm(X - (D @ Z)) ** 2) + (lamb * np.sum(np.abs(Z)))\n",
    "        if np.linalg.norm(grad) <= threshold:\n",
    "            break\n",
    "    \n",
    "    # print(f\"Loss at Iteration {iter} = {loss}, Magnitude of Gradient = {np.linalg.norm(grad)}\")\n",
    "    return Z\n",
    "\n",
    "#Normalize a matrix D such that its column norms <= 1\n",
    "def normalize(D: np.ndarray):\n",
    "    norms = np.linalg.norm(D, axis=0)  # Calculate column norms\n",
    "    mask = norms > 1  # Find columns with norms > 1\n",
    "    D[:, mask] /= norms[mask]  # Normalize only columns with norms > 1\n",
    "    return D\n",
    "\n",
    "## Solve the Quadratic Programming Portion of Joint Dictionary Training\n",
    "## Goal: Find D that minimizes || X - DZ||_2^2\n",
    "def quadratic_programming(X: np.ndarray, Z: np.ndarray, Dr, Dc, step_size, threshold, max_iter):    \n",
    "    #Run Projected Gradient Descent\n",
    "    D = np.random.normal(size = (Dr, Dc))\n",
    "    D = normalize(D)\n",
    "    loss = (np.linalg.norm(X - (D @ Z)) ** 2) / (np.linalg.norm(X))\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        grad = (-2 * (X @ Z.T)) + (2 * (D @ Z @ Z.T))\n",
    "        \n",
    "        D = D - (step_size * grad)\n",
    "        D = normalize(D)\n",
    "        loss = (np.linalg.norm(X - (D @ Z)) ** 2) / (np.linalg.norm(X))\n",
    "        \n",
    "        if np.linalg.norm(grad) <= threshold:\n",
    "            break\n",
    "    \n",
    "        # print(f\"Loss at Iteration {iter} = {loss}, Magnitude of Gradient = {np.linalg.norm(grad)}\")\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance-Thresholding Based Patch Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prune patches whose variances are below a certain threshold\n",
    "#Xh: High Resolution Patch\n",
    "#Xl: Low Resolution Patch\n",
    "#variance_threshold: variance threshold\n",
    "def patch_pruning(Xh, Xl, variance_threshold):\n",
    "    patch_variances = np.var(Xh, 0)\n",
    "    idx = patch_variances > variance_threshold\n",
    "    Xh = Xh[:, idx]\n",
    "    Xl = Xl[:, idx]\n",
    "    return Xh, Xl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to Randomly Generate Patches\n",
      "Number of Images From Training Dataset we have Sampled:  69\n",
      "Going to Prune Patches\n",
      "(25, 9891) (100, 9891)\n",
      "Going to Jointly Train Dictionaries\n",
      "STARTING TRAINING\n",
      "Xc shape: (125, 9891)\n",
      "Dc shape: (125, 512)\n",
      "Iteration 1/100 Linear Programming Stat: 0.979476224859944\n",
      "Iteration 1/100 Quadratic Programming Stat: 0.9400659259233721\n",
      "Iteration 2/100 Linear Programming Stat: 0.6876072575921763\n",
      "Iteration 2/100 Quadratic Programming Stat: 0.6765388343323963\n",
      "Iteration 3/100 Linear Programming Stat: 0.6396836093374825\n",
      "Iteration 3/100 Quadratic Programming Stat: 0.6402058590369595\n",
      "Iteration 4/100 Linear Programming Stat: 0.6304996762814757\n",
      "Iteration 4/100 Quadratic Programming Stat: 0.6289084603495972\n",
      "Iteration 5/100 Linear Programming Stat: 0.6216188604904287\n",
      "Iteration 5/100 Quadratic Programming Stat: 0.6305257769746347\n",
      "Iteration 6/100 Linear Programming Stat: 0.6195943566010342\n",
      "Iteration 6/100 Quadratic Programming Stat: 0.6266077271187577\n",
      "Iteration 7/100 Linear Programming Stat: 0.6205536623998565\n",
      "Iteration 7/100 Quadratic Programming Stat: 0.6325831750263603\n",
      "Iteration 8/100 Linear Programming Stat: 0.6200661562539743\n",
      "Iteration 8/100 Quadratic Programming Stat: 0.6285656183452969\n",
      "Iteration 9/100 Linear Programming Stat: 0.6192794843118536\n",
      "Iteration 9/100 Quadratic Programming Stat: 0.6301018255616607\n",
      "Iteration 10/100 Linear Programming Stat: 0.6234438161205376\n",
      "Iteration 10/100 Quadratic Programming Stat: 0.630977320512151\n",
      "Iteration 11/100 Linear Programming Stat: 0.6171369547598019\n",
      "Iteration 11/100 Quadratic Programming Stat: 0.629832450323773\n",
      "Iteration 12/100 Linear Programming Stat: 0.6220547059561584\n",
      "Iteration 12/100 Quadratic Programming Stat: 0.6318530567671153\n",
      "Iteration 13/100 Linear Programming Stat: 0.620952918648691\n",
      "Iteration 13/100 Quadratic Programming Stat: 0.6320313364047763\n",
      "Iteration 14/100 Linear Programming Stat: 0.6232442388181082\n",
      "Iteration 14/100 Quadratic Programming Stat: 0.6340406693529229\n",
      "Iteration 15/100 Linear Programming Stat: 0.6221288055094133\n",
      "Iteration 15/100 Quadratic Programming Stat: 0.6325390983549998\n",
      "Iteration 16/100 Linear Programming Stat: 0.6255467446162328\n",
      "Iteration 16/100 Quadratic Programming Stat: 0.6328748275247452\n",
      "Iteration 17/100 Linear Programming Stat: 0.6192179125718499\n",
      "Iteration 17/100 Quadratic Programming Stat: 0.6314547367821726\n",
      "Iteration 18/100 Linear Programming Stat: 0.6212213673102714\n",
      "Iteration 18/100 Quadratic Programming Stat: 0.6309775014003047\n",
      "Iteration 19/100 Linear Programming Stat: 0.620758498996442\n",
      "Iteration 19/100 Quadratic Programming Stat: 0.6348382455284509\n",
      "Iteration 20/100 Linear Programming Stat: 0.6247795759725929\n",
      "Iteration 20/100 Quadratic Programming Stat: 0.6396193206253475\n",
      "Iteration 21/100 Linear Programming Stat: 0.6234272068263196\n",
      "Iteration 21/100 Quadratic Programming Stat: 0.6363655127225494\n",
      "Iteration 22/100 Linear Programming Stat: 0.6234549256265084\n",
      "Iteration 22/100 Quadratic Programming Stat: 0.6344276892008948\n",
      "Iteration 23/100 Linear Programming Stat: 0.6247817049876645\n",
      "Iteration 23/100 Quadratic Programming Stat: 0.6411087780736023\n",
      "Iteration 24/100 Linear Programming Stat: 0.6264428960423967\n",
      "Iteration 24/100 Quadratic Programming Stat: 0.6380537897823286\n",
      "Iteration 25/100 Linear Programming Stat: 0.6259741256239868\n",
      "Iteration 25/100 Quadratic Programming Stat: 0.6361830635505681\n",
      "Iteration 26/100 Linear Programming Stat: 0.628027707051039\n",
      "Iteration 26/100 Quadratic Programming Stat: 0.6395112217215533\n",
      "Iteration 27/100 Linear Programming Stat: 0.6258613550059993\n",
      "Iteration 27/100 Quadratic Programming Stat: 0.6387270809736711\n",
      "Iteration 28/100 Linear Programming Stat: 0.6246349285781794\n",
      "Iteration 28/100 Quadratic Programming Stat: 0.6350288426515786\n",
      "Iteration 29/100 Linear Programming Stat: 0.6250368198511831\n",
      "Iteration 29/100 Quadratic Programming Stat: 0.6336329668921576\n",
      "Iteration 30/100 Linear Programming Stat: 0.6208813469234888\n",
      "Iteration 30/100 Quadratic Programming Stat: 0.6430073210509535\n",
      "Iteration 31/100 Linear Programming Stat: 0.6274250395219385\n",
      "Iteration 31/100 Quadratic Programming Stat: 0.6376139351547483\n",
      "Iteration 32/100 Linear Programming Stat: 0.6267865591661291\n",
      "Iteration 32/100 Quadratic Programming Stat: 0.6371202234285797\n",
      "Iteration 33/100 Linear Programming Stat: 0.6246294364101022\n",
      "Iteration 33/100 Quadratic Programming Stat: 0.6359770627848699\n",
      "Iteration 34/100 Linear Programming Stat: 0.6257372546693631\n",
      "Iteration 34/100 Quadratic Programming Stat: 0.6378412264647656\n",
      "Iteration 35/100 Linear Programming Stat: 0.6243908494602315\n",
      "Iteration 35/100 Quadratic Programming Stat: 0.6375984208391526\n",
      "Iteration 36/100 Linear Programming Stat: 0.6254366940655464\n",
      "Iteration 36/100 Quadratic Programming Stat: 0.6358654767724351\n",
      "Iteration 37/100 Linear Programming Stat: 0.6263382820877467\n",
      "Iteration 37/100 Quadratic Programming Stat: 0.6346458751435318\n",
      "Iteration 38/100 Linear Programming Stat: 0.624220866809165\n",
      "Iteration 38/100 Quadratic Programming Stat: 0.6326224673780262\n",
      "Iteration 39/100 Linear Programming Stat: 0.6215424509076021\n",
      "Iteration 39/100 Quadratic Programming Stat: 0.6371993159727797\n",
      "Iteration 40/100 Linear Programming Stat: 0.6283008539076059\n",
      "Iteration 40/100 Quadratic Programming Stat: 0.6345134229360292\n",
      "Iteration 41/100 Linear Programming Stat: 0.6227231157858384\n",
      "Iteration 41/100 Quadratic Programming Stat: 0.639452566114352\n",
      "Iteration 42/100 Linear Programming Stat: 0.6296117543622946\n",
      "Iteration 42/100 Quadratic Programming Stat: 0.6385102602266531\n",
      "Iteration 43/100 Linear Programming Stat: 0.6224088106216117\n",
      "Iteration 43/100 Quadratic Programming Stat: 0.6390331795217219\n",
      "Iteration 44/100 Linear Programming Stat: 0.6313337017139445\n",
      "Iteration 44/100 Quadratic Programming Stat: 0.6375489402516252\n",
      "Iteration 45/100 Linear Programming Stat: 0.6231974085708478\n",
      "Iteration 45/100 Quadratic Programming Stat: 0.6383995771567047\n",
      "Iteration 46/100 Linear Programming Stat: 0.6296672110069044\n",
      "Iteration 46/100 Quadratic Programming Stat: 0.6390871155098365\n",
      "Iteration 47/100 Linear Programming Stat: 0.6256987483193145\n",
      "Iteration 47/100 Quadratic Programming Stat: 0.6410814809899339\n",
      "Iteration 48/100 Linear Programming Stat: 0.6309837394755621\n",
      "Iteration 48/100 Quadratic Programming Stat: 0.6391125431277895\n",
      "Iteration 49/100 Linear Programming Stat: 0.6279223005033803\n",
      "Iteration 49/100 Quadratic Programming Stat: 0.6383460418621771\n",
      "Iteration 50/100 Linear Programming Stat: 0.6291940702313749\n",
      "Iteration 50/100 Quadratic Programming Stat: 0.6424429491411806\n",
      "Iteration 51/100 Linear Programming Stat: 0.6263757093497337\n",
      "Iteration 51/100 Quadratic Programming Stat: 0.63833381727038\n",
      "Iteration 52/100 Linear Programming Stat: 0.6297556906322908\n",
      "Iteration 52/100 Quadratic Programming Stat: 0.6409280343811853\n",
      "Iteration 53/100 Linear Programming Stat: 0.6311668751762123\n",
      "Iteration 53/100 Quadratic Programming Stat: 0.6407557843036062\n",
      "Iteration 54/100 Linear Programming Stat: 0.6283804657824669\n",
      "Iteration 54/100 Quadratic Programming Stat: 0.6420317309576662\n",
      "Iteration 55/100 Linear Programming Stat: 0.6333128690739831\n",
      "Iteration 55/100 Quadratic Programming Stat: 0.638056470408026\n",
      "Iteration 56/100 Linear Programming Stat: 0.6277869909289595\n",
      "Iteration 56/100 Quadratic Programming Stat: 0.6379545970171815\n",
      "Iteration 57/100 Linear Programming Stat: 0.6279505299641572\n",
      "Iteration 57/100 Quadratic Programming Stat: 0.6437154445064239\n",
      "Iteration 58/100 Linear Programming Stat: 0.6339496868455659\n",
      "Iteration 58/100 Quadratic Programming Stat: 0.6436364406609761\n",
      "Iteration 59/100 Linear Programming Stat: 0.6298584165760207\n",
      "Iteration 59/100 Quadratic Programming Stat: 0.6422795757821613\n",
      "Iteration 60/100 Linear Programming Stat: 0.6326446474568476\n",
      "Iteration 60/100 Quadratic Programming Stat: 0.6442459917930808\n",
      "Iteration 61/100 Linear Programming Stat: 0.6302596360477525\n",
      "Iteration 61/100 Quadratic Programming Stat: 0.6475675223382461\n",
      "Iteration 62/100 Linear Programming Stat: 0.6319197614474551\n",
      "Iteration 62/100 Quadratic Programming Stat: 0.6440780668942356\n",
      "Iteration 63/100 Linear Programming Stat: 0.6350708584351744\n",
      "Iteration 63/100 Quadratic Programming Stat: 0.6489663029052157\n",
      "Iteration 64/100 Linear Programming Stat: 0.6351762900158892\n",
      "Iteration 64/100 Quadratic Programming Stat: 0.6437677017143768\n",
      "Iteration 65/100 Linear Programming Stat: 0.6313364593793973\n",
      "Iteration 65/100 Quadratic Programming Stat: 0.6493907189157552\n",
      "Iteration 66/100 Linear Programming Stat: 0.6366967827222327\n",
      "Iteration 66/100 Quadratic Programming Stat: 0.6466793972501342\n",
      "Iteration 67/100 Linear Programming Stat: 0.6329005764376994\n",
      "Iteration 67/100 Quadratic Programming Stat: 0.6465763003083641\n",
      "Iteration 68/100 Linear Programming Stat: 0.6386086185453353\n",
      "Iteration 68/100 Quadratic Programming Stat: 0.6428367023813233\n",
      "Iteration 69/100 Linear Programming Stat: 0.6321398550898583\n",
      "Iteration 69/100 Quadratic Programming Stat: 0.6479931055813151\n",
      "Iteration 70/100 Linear Programming Stat: 0.6401663788795368\n",
      "Iteration 70/100 Quadratic Programming Stat: 0.6418149007713051\n",
      "Iteration 71/100 Linear Programming Stat: 0.6294281093578967\n",
      "Iteration 71/100 Quadratic Programming Stat: 0.6500728355004793\n",
      "Iteration 72/100 Linear Programming Stat: 0.6346383714052495\n",
      "Iteration 72/100 Quadratic Programming Stat: 0.6525259504948331\n",
      "Iteration 73/100 Linear Programming Stat: 0.6400286857746913\n",
      "Iteration 73/100 Quadratic Programming Stat: 0.6559459316535997\n",
      "Iteration 74/100 Linear Programming Stat: 0.6370534721909275\n",
      "Iteration 74/100 Quadratic Programming Stat: 0.6569126527112366\n",
      "Iteration 75/100 Linear Programming Stat: 0.6415965037418209\n",
      "Iteration 75/100 Quadratic Programming Stat: 0.6518006923492827\n",
      "Iteration 76/100 Linear Programming Stat: 0.6356917867665594\n",
      "Iteration 76/100 Quadratic Programming Stat: 0.649834937479534\n",
      "Iteration 77/100 Linear Programming Stat: 0.6369800488663322\n",
      "Iteration 77/100 Quadratic Programming Stat: 0.6514653599375321\n",
      "Iteration 78/100 Linear Programming Stat: 0.6396198377633044\n",
      "Iteration 78/100 Quadratic Programming Stat: 0.6503625888980891\n",
      "Iteration 79/100 Linear Programming Stat: 0.6339488201571428\n",
      "Iteration 79/100 Quadratic Programming Stat: 0.6477378920140029\n",
      "Iteration 80/100 Linear Programming Stat: 0.6346240530398924\n",
      "Iteration 80/100 Quadratic Programming Stat: 0.6527885314147949\n",
      "Iteration 81/100 Linear Programming Stat: 0.6417265618857222\n",
      "Iteration 81/100 Quadratic Programming Stat: 0.6477490961433525\n",
      "Iteration 82/100 Linear Programming Stat: 0.638081059805124\n",
      "Iteration 82/100 Quadratic Programming Stat: 0.6463619130873232\n",
      "Iteration 83/100 Linear Programming Stat: 0.6330181873725804\n",
      "Iteration 83/100 Quadratic Programming Stat: 0.6466461263570965\n",
      "Iteration 84/100 Linear Programming Stat: 0.6398417604373474\n",
      "Iteration 84/100 Quadratic Programming Stat: 0.6477014058439984\n",
      "Iteration 85/100 Linear Programming Stat: 0.6365841555561158\n",
      "Iteration 85/100 Quadratic Programming Stat: 0.6480165488621485\n",
      "Iteration 86/100 Linear Programming Stat: 0.6373454898700649\n",
      "Iteration 86/100 Quadratic Programming Stat: 0.6531996626700058\n",
      "Iteration 87/100 Linear Programming Stat: 0.6386450111254572\n",
      "Iteration 87/100 Quadratic Programming Stat: 0.6495837979827265\n",
      "Iteration 88/100 Linear Programming Stat: 0.6361643307707757\n",
      "Iteration 88/100 Quadratic Programming Stat: 0.6581107143159832\n",
      "Iteration 89/100 Linear Programming Stat: 0.6442313712808022\n",
      "Iteration 89/100 Quadratic Programming Stat: 0.6564904228833238\n",
      "Iteration 90/100 Linear Programming Stat: 0.6389231525501243\n",
      "Iteration 90/100 Quadratic Programming Stat: 0.655610496997582\n",
      "Iteration 91/100 Linear Programming Stat: 0.6463264427318332\n",
      "Iteration 91/100 Quadratic Programming Stat: 0.647436337334531\n",
      "Iteration 92/100 Linear Programming Stat: 0.6330889240699086\n",
      "Iteration 92/100 Quadratic Programming Stat: 0.652700373758151\n",
      "Iteration 93/100 Linear Programming Stat: 0.6458594727938509\n",
      "Iteration 93/100 Quadratic Programming Stat: 0.6465422084459344\n",
      "Iteration 94/100 Linear Programming Stat: 0.6336645377746257\n",
      "Iteration 94/100 Quadratic Programming Stat: 0.6564142596423574\n",
      "Iteration 95/100 Linear Programming Stat: 0.643464952351682\n",
      "Iteration 95/100 Quadratic Programming Stat: 0.650928631442697\n",
      "Iteration 96/100 Linear Programming Stat: 0.6340856325288292\n",
      "Iteration 96/100 Quadratic Programming Stat: 0.6569239562171256\n",
      "Iteration 97/100 Linear Programming Stat: 0.6481776887851541\n",
      "Iteration 97/100 Quadratic Programming Stat: 0.6526808304593089\n",
      "Iteration 98/100 Linear Programming Stat: 0.6368890559526706\n",
      "Iteration 98/100 Quadratic Programming Stat: 0.6554938032300385\n",
      "Iteration 99/100 Linear Programming Stat: 0.6474508633515351\n",
      "Iteration 99/100 Quadratic Programming Stat: 0.6492170877247594\n",
      "Iteration 100/100 Linear Programming Stat: 0.6337487220270537\n",
      "Iteration 100/100 Quadratic Programming Stat: 0.6547174004260877\n",
      "Time taken to Jointly Train Dictionaries: 641.8607921600342\n"
     ]
    }
   ],
   "source": [
    "training_image_path = \"../Data/Training\" #path that has all training images\n",
    "\n",
    "dict_size = 512 #Dictionary Size will be 512\n",
    "lamb = 0.15 #sparsity regularization\n",
    "patch_size = 5 #size of patches will be 3 x 3\n",
    "nSmp = 10000 #number of patches to sample\n",
    "upscale = 4 #upscale factor\n",
    "\n",
    "#randomly sample patches from training images\n",
    "print(\"Going to Randomly Generate Patches\")\n",
    "Xh, Xl = rnd_smp_patch(training_image_path, '.bmp', patch_size, nSmp, upscale)\n",
    "\n",
    "#Prune patches with small variance\n",
    "print(\"Going to Prune Patches\")\n",
    "Xh, Xl = patch_pruning(Xh, Xl, variance_threshold = 1)\n",
    "\n",
    "print(Xh.shape, Xl.shape)\n",
    "\n",
    "#Joint Dictionary Training\n",
    "start_time = time.time()\n",
    "step_size = 0.0001\n",
    "variance_threshold = 0.0001\n",
    "max_iter = 100\n",
    "print(\"Going to Jointly Train Dictionaries\")\n",
    "Dc = train_coupled_dict(Xh, Xl, dict_size, step_size, lamb, variance_threshold, max_iter)\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken to Jointly Train Dictionaries: {elapsed_time}\")\n",
    "\n",
    "N, M = Xh.shape[0], Xl.shape[0]\n",
    "Dh, Dl = Dc[:N], Dc[N:]\n",
    "\n",
    "#Save Dh and Dl to npy files\n",
    "np.save('Dh.npy', Dh)\n",
    "np.save('Dl.npy', Dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroimaging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
