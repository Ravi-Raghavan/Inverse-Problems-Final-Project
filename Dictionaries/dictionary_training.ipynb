{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary Training\n",
    "\n",
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn import linear_model\n",
    "from scipy import signal\n",
    "from scipy.signal import convolve2d\n",
    "import cvxpy as cp\n",
    "import time\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Patch from Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hIm: high resolution image\n",
    "#patch_size: patch size that we want to retrieve from image\n",
    "#num_patches: number of patches we want to sample\n",
    "#upscale: upscale factor from low resolution image to high resolution image\n",
    "def sample_patches(hIm, patch_size, num_patches, upscale):\n",
    "    #Convert RBG to Grayscale Images\n",
    "    if hIm.shape[2] == 3:\n",
    "        hIm = cv2.cvtColor(hIm, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "    #Blur the High Resolution Image a bit\n",
    "    blur_kernel = np.ones(shape = (3, 3)) / 9\n",
    "    blurred_hIm = convolve2d(hIm, blur_kernel, mode = 'same')\n",
    "        \n",
    "    #Generate Low Resolution Image\n",
    "    lIm = cv2.resize(blurred_hIm, tuple(int(x * (1/upscale)) for x in blurred_hIm.shape)[::-1], interpolation = cv2.INTER_NEAREST)\n",
    "    lIm = cv2.resize(lIm, blurred_hIm.shape[::-1], interpolation = cv2.INTER_NEAREST)\n",
    "    \n",
    "    #Get dimensions of High Resolution Image\n",
    "    nrow, ncol = hIm.shape\n",
    "    \n",
    "    #Get posible values of (x, y) that is top left corner of patch. The (x,y) coordinates are in the coordinate space of the High Resolution Image\n",
    "    x = np.random.permutation(np.arange(0, nrow - 2 * patch_size - 1)) + patch_size\n",
    "    y = np.random.permutation(np.arange(0, ncol - 2 * patch_size - 1)) + patch_size\n",
    "    \n",
    "    #Generated Meshgrid\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    #Flatten X and Y column wise\n",
    "    xrow = X.flatten(order = 'F')\n",
    "    ycol = Y.flatten(order = 'F')\n",
    "    \n",
    "    #If we have less patches than potential (x, y) coordinates, we have to truncate the list of (x, y) coordinates\n",
    "    if num_patches < len(xrow):\n",
    "        xrow = xrow[: num_patches]\n",
    "        ycol = ycol[: num_patches]\n",
    "    \n",
    "    num_patches = len(xrow)\n",
    "    \n",
    "    #Store High and Low Resolution Patches\n",
    "    H = np.zeros(shape = (patch_size ** 2, num_patches))\n",
    "    L = np.zeros(shape = (4 * (patch_size ** 2), num_patches))\n",
    "    \n",
    "    #Compute first order derivatives\n",
    "    hf1 = np.array([-1,0,1]).reshape((1, -1))\n",
    "    vf1 = hf1.T\n",
    "    \n",
    "    lImG11 = signal.convolve2d(lIm, hf1[::-1, ::-1],'same') #row wise 1st order derivative\n",
    "    lImG12 = signal.convolve2d(lIm, vf1[::-1, ::-1],'same') #column wise 1st order derivative\n",
    "    \n",
    "    #Compute second order derivatives\n",
    "    hf2 = np.array([1,0,-2,0,1]).reshape((1, -1))\n",
    "    vf2 = hf2.T\n",
    "    \n",
    "    lImG21 = signal.convolve2d(lIm, hf2[::-1, ::-1], 'same') #row wise 2nd order derivative\n",
    "    lImG22 = signal.convolve2d(lIm, vf2[::-1, ::-1], 'same') #column wise 2nd order derivative\n",
    "    \n",
    "    #Extract Patches\n",
    "    for idx in range(num_patches):\n",
    "        row, col = xrow[idx], ycol[idx]\n",
    "        \n",
    "        #Get the patch from High Resolution Image\n",
    "        Hpatch = hIm[row: row + patch_size, col: col + patch_size].flatten(order = 'F')\n",
    "        H[:, idx] = Hpatch - np.mean(Hpatch) #Store High Resolution Patch\n",
    "        \n",
    "        #Get the patch from Low Resolution Image\n",
    "        Lpatch1 = lImG11[row:row+patch_size,col:col+patch_size].flatten(order = 'F')\n",
    "        Lpatch2 = lImG12[row:row+patch_size,col:col+patch_size].flatten(order = 'F')\n",
    "        Lpatch3 = lImG21[row:row+patch_size,col:col+patch_size].flatten(order = 'F')\n",
    "        Lpatch4 = lImG22[row:row+patch_size,col:col+patch_size].flatten(order = 'F')\n",
    "\n",
    "        Lpatch = np.concatenate((Lpatch1, Lpatch2, Lpatch3, Lpatch4))\n",
    "        L[:, idx] = Lpatch #Store Low Resolution Patch\n",
    "    \n",
    "    return H, L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly Sample Patches From Training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly Sample Patches\n",
    "#img_path: path of image\n",
    "#img_type: type of image\n",
    "#patch_size: size of patch\n",
    "#num_patch: number of patches we want to sample\n",
    "#upscale: upscale factor from low resolution image to high resolution image\n",
    "def rnd_smp_patch(img_path, img_type, patch_size, num_patch, upscale):\n",
    "    #image directory with training image path\n",
    "    img_dir = [file for file in os.listdir(img_path) if file.endswith(img_type)]\n",
    "        \n",
    "    Xh = [] #Store High Resolution Patches\n",
    "    Xl = [] #Store Low Resolution Patches\n",
    "    \n",
    "    img_num = len(img_dir) #Total number of images\n",
    "    nper_img = np.zeros(shape = (img_num, )) #number of patches per image\n",
    "    print(\"Number of Images From Training Dataset we have Sampled: \", img_num)\n",
    "    \n",
    "    #Store total size of all images\n",
    "    for idx in range(img_num):\n",
    "        im = cv2.imread(os.path.join(img_path, img_dir[idx]))\n",
    "        nper_img[idx] = im.size\n",
    "    \n",
    "    nper_img = np.floor(nper_img * num_patch / np.sum(nper_img)).astype(int) #number of patches per image\n",
    "    \n",
    "    #iterate through images\n",
    "    for idx in range(img_num):\n",
    "        patch_num = nper_img[idx] #number of patches from this image to select\n",
    "        im = cv2.imread(os.path.join(img_path, img_dir[idx])) #Get image\n",
    "        \n",
    "        #Sample the Patches\n",
    "        H, L = sample_patches(im, patch_size, patch_num, upscale)\n",
    "                \n",
    "        #Append to Xh and Xl\n",
    "        Xh.append(H)\n",
    "        Xl.append(L)\n",
    "    \n",
    "    Xh, Xl = np.concatenate(Xh, axis = 1), np.concatenate(Xl, axis = 1) #Concatenate Patches into numpy array\n",
    "    return Xh, Xl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jointly Train Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jointly train dictionaries\n",
    "#Xh: High Resolution Patches\n",
    "#Xl: Low Resolution Patches\n",
    "#dict_size: size of dictionary\n",
    "#step size: step size for Quadratic Programming step\n",
    "#lamb: lambda for linear programming step\n",
    "#threshold: threshold for iterations in quadratic programming\n",
    "#max_iter: maximum number of iterations\n",
    "def train_coupled_dict(Xh, Xl, dict_size, step_size, lamb, threshold, max_iter):\n",
    "    print(\"STARTING TRAINING\")\n",
    "    \n",
    "    #Get shape of Patch Data\n",
    "    N, M = Xh.shape[0], Xl.shape[0]\n",
    "    \n",
    "    #Get Constants\n",
    "    a1 = 1 / np.sqrt(N)\n",
    "    a2 = 1 / np.sqrt(M)\n",
    "    \n",
    "    #Initialize Xc\n",
    "    Xc = np.concatenate((a1 * Xh, a2 * Xl), axis = 0)\n",
    "    print(f\"Xc shape: {Xc.shape}\")\n",
    "    \n",
    "    #Initialize D as a random Gaussian Matrix\n",
    "    Dc = np.random.normal(size = (N + M, dict_size))\n",
    "    Dc = normalize(Dc)\n",
    "    print(f\"Dc shape: {Dc.shape}\")\n",
    "    \n",
    "    #cap maximum iterations at max_iter\n",
    "    for iter in range(max_iter):\n",
    "        Z = lasso_optimization(Xc, Dc, lamb) #Run lasso Optimization\n",
    "        Xc_pred = Dc @ Z #Get XC_pred to compute loss metric\n",
    "        print(f\"Iteration {iter + 1}/{max_iter} Linear Programming Stat: {np.linalg.norm(Xc - Xc_pred) / np.linalg.norm(Xc)}\")\n",
    "        \n",
    "        Dc = quadratic_programming(Xc, Z, Dc.shape[0], Dc.shape[1], step_size, threshold, 30) #Run Quadratic Programming Step\n",
    "        Xc_pred = Dc @ Z #Get XC_pred to compute loss metric\n",
    "        print(f\"Iteration {iter + 1}/{max_iter} Quadratic Programming Stat: {np.linalg.norm(Xc - Xc_pred) / np.linalg.norm(Xc)}\")\n",
    "    \n",
    "    return Dc\n",
    "\n",
    "#Lasso Optimization\n",
    "#Goal: Solve for value of Z to minimize ||Xc - Dc Z||_2^2 + lamb * ||Z||_1\n",
    "def lasso_optimization(Xc, Dc, lamb):\n",
    "    clf = linear_model.Lasso(alpha = lamb, max_iter = 100000, fit_intercept = False)\n",
    "    clf.fit(Dc, Xc)\n",
    "    return clf.coef_.T\n",
    "        \n",
    "#prox operator given x and alpha\n",
    "def prox(x, alpha):\n",
    "    return np.piecewise(x, [x < -alpha, (x >= -alpha) & (x <= alpha), x >= alpha], [lambda x: x + alpha, 0, lambda x: x - alpha])\n",
    "\n",
    "## Solve the Linear Programming Portion of Joint Dictionary Training\n",
    "## Goal: Find Z that minimizes || X - DZ||_2^2 + lambda * ||Z||_1\n",
    "def linear_programming(X: np.ndarray, D: np.ndarray, Zr, Zc, step_size, lamb, threshold, max_iter):\n",
    "    Z = np.random.normal(size = (Zr, Zc))\n",
    "    \n",
    "    #Run Proximal Gradient Descent\n",
    "    loss = (np.linalg.norm(X - (D @ Z)) ** 2) + (lamb * np.sum(np.abs(Z)))\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        grad = (-2 * (D.T @ X)) + (2 * (D.T @ D @ Z))\n",
    "        \n",
    "        #Update Z\n",
    "        Z = Z - (step_size * grad)\n",
    "        Z = prox(Z, step_size * lamb)\n",
    "        \n",
    "        loss = (np.linalg.norm(X - (D @ Z)) ** 2) + (lamb * np.sum(np.abs(Z)))\n",
    "        if np.linalg.norm(grad) <= threshold:\n",
    "            break\n",
    "    \n",
    "    # print(f\"Loss at Iteration {iter} = {loss}, Magnitude of Gradient = {np.linalg.norm(grad)}\")\n",
    "    return Z\n",
    "\n",
    "#Normalize a matrix D such that its column norms <= 1\n",
    "def normalize(D: np.ndarray):\n",
    "    norms = np.linalg.norm(D, axis=0)  # Calculate column norms\n",
    "    mask = norms > 1  # Find columns with norms > 1\n",
    "    D[:, mask] /= norms[mask]  # Normalize only columns with norms > 1\n",
    "    return D\n",
    "\n",
    "## Solve the Quadratic Programming Portion of Joint Dictionary Training\n",
    "## Goal: Find D that minimizes || X - DZ||_2^2\n",
    "def quadratic_programming(X: np.ndarray, Z: np.ndarray, Dr, Dc, step_size, threshold, max_iter):    \n",
    "    #Run Projected Gradient Descent\n",
    "    D = np.random.normal(size = (Dr, Dc))\n",
    "    D = normalize(D)\n",
    "    loss = (np.linalg.norm(X - (D @ Z)) ** 2) / (np.linalg.norm(X))\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        grad = (-2 * (X @ Z.T)) + (2 * (D @ Z @ Z.T))\n",
    "        \n",
    "        D = D - (step_size * grad)\n",
    "        D = normalize(D)\n",
    "        loss = (np.linalg.norm(X - (D @ Z)) ** 2) / (np.linalg.norm(X))\n",
    "        \n",
    "        if np.linalg.norm(grad) <= threshold:\n",
    "            break\n",
    "    \n",
    "        # print(f\"Loss at Iteration {iter} = {loss}, Magnitude of Gradient = {np.linalg.norm(grad)}\")\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance-Thresholding Based Patch Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prune patches whose variances are below a certain threshold\n",
    "#Xh: High Resolution Patch\n",
    "#Xl: Low Resolution Patch\n",
    "#variance_threshold: variance threshold\n",
    "def patch_pruning(Xh, Xl, variance_threshold):\n",
    "    patch_variances = np.var(Xh, 0)\n",
    "    idx = patch_variances > variance_threshold\n",
    "    Xh = Xh[:, idx]\n",
    "    Xl = Xl[:, idx]\n",
    "    return Xh, Xl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to Randomly Generate Patches\n",
      "Number of Images From Training Dataset we have Sampled:  69\n",
      "Going to Prune Patches\n",
      "(25, 9888) (100, 9888)\n",
      "Going to Jointly Train Dictionaries\n",
      "STARTING TRAINING\n",
      "Xc shape: (125, 9888)\n",
      "Dc shape: (125, 512)\n",
      "Iteration 1/10 Linear Programming Stat: 0.9788898842951262\n",
      "Iteration 1/10 Quadratic Programming Stat: 0.9397385749116924\n",
      "Iteration 2/10 Linear Programming Stat: 0.7132265317645815\n",
      "Iteration 2/10 Quadratic Programming Stat: 0.694919479996787\n",
      "Iteration 3/10 Linear Programming Stat: 0.6590612304242257\n",
      "Iteration 3/10 Quadratic Programming Stat: 0.6545793492907397\n",
      "Iteration 4/10 Linear Programming Stat: 0.6436152338585733\n",
      "Iteration 4/10 Quadratic Programming Stat: 0.6424265986867981\n",
      "Iteration 5/10 Linear Programming Stat: 0.6372965643571701\n",
      "Iteration 5/10 Quadratic Programming Stat: 0.6383524212626221\n",
      "Iteration 6/10 Linear Programming Stat: 0.6341268198519402\n",
      "Iteration 6/10 Quadratic Programming Stat: 0.6376219764542974\n",
      "Iteration 7/10 Linear Programming Stat: 0.6339203343158872\n",
      "Iteration 7/10 Quadratic Programming Stat: 0.637231017024477\n",
      "Iteration 8/10 Linear Programming Stat: 0.6321826737191094\n",
      "Iteration 8/10 Quadratic Programming Stat: 0.636053390507071\n",
      "Iteration 9/10 Linear Programming Stat: 0.6331742999451823\n",
      "Iteration 9/10 Quadratic Programming Stat: 0.6339845765037442\n",
      "Iteration 10/10 Linear Programming Stat: 0.6316658546242907\n",
      "Iteration 10/10 Quadratic Programming Stat: 0.6360354594903499\n",
      "Time taken to Jointly Train Dictionaries: 81.05862402915955\n"
     ]
    }
   ],
   "source": [
    "training_image_path = \"../Data/Training\" #path that has all training images\n",
    "\n",
    "dict_size = 512 #Dictionary Size will be 512\n",
    "lamb = 0.15 #sparsity regularization\n",
    "patch_size = 5 #size of patches will be 3 x 3\n",
    "nSmp = 10000 #number of patches to sample\n",
    "upscale = 4 #upscale factor\n",
    "\n",
    "#randomly sample patches from training images\n",
    "print(\"Going to Randomly Generate Patches\")\n",
    "Xh, Xl = rnd_smp_patch(training_image_path, '.bmp', patch_size, nSmp, upscale)\n",
    "\n",
    "#Prune patches with small variance\n",
    "print(\"Going to Prune Patches\")\n",
    "Xh, Xl = patch_pruning(Xh, Xl, variance_threshold = 1)\n",
    "\n",
    "print(Xh.shape, Xl.shape)\n",
    "\n",
    "#Joint Dictionary Training\n",
    "start_time = time.time()\n",
    "step_size = 0.0001\n",
    "variance_threshold = 0.0001\n",
    "max_iter = 10\n",
    "print(\"Going to Jointly Train Dictionaries\")\n",
    "Dc = train_coupled_dict(Xh, Xl, dict_size, step_size, lamb, variance_threshold, max_iter)\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken to Jointly Train Dictionaries: {elapsed_time}\")\n",
    "\n",
    "N, M = Xh.shape[0], Xl.shape[0]\n",
    "Dh, Dl = Dc[:N], Dc[N:]\n",
    "\n",
    "#Save Dh and Dl to npy files\n",
    "np.save('Dh.npy', Dh)\n",
    "np.save('Dl.npy', Dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroimaging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
