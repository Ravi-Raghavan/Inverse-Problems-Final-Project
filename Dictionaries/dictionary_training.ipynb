{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary Training\n",
    "\n",
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn import linear_model\n",
    "from scipy import signal\n",
    "from scipy.signal import convolve2d\n",
    "import scipy.io as sio\n",
    "import cvxpy as cp\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .mat to .npy Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .mat file\n",
    "mat_data = sio.loadmat('D_512_0.15_5.mat')\n",
    "\n",
    "Dh, Dl = mat_data['Dh'], mat_data['Dl']\n",
    "print(Dh.shape, Dl.shape)\n",
    "\n",
    "# Save Dh and Dl as .npy files\n",
    "np.save('Dh_512_0.15_5.npy', Dh)\n",
    "np.save('Dl_512_0.15_5.npy', Dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Patches from Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_patches(im, patch_size, patch_num, upscale):\n",
    "    #Initialize the High Resolution Image\n",
    "    hIm = im\n",
    "    if im.shape[2] == 3:\n",
    "        hIm = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "    #Blur the High Resolution Image a bit\n",
    "    blur_kernel = np.ones(shape = (3, 3)) / 9\n",
    "    blurred_hIm = convolve2d(hIm, blur_kernel, mode = 'same')\n",
    "        \n",
    "    #Generate Low Resolution Image\n",
    "    lIm = cv2.resize(blurred_hIm, tuple(int(x * (1/upscale)) for x in blurred_hIm.shape)[::-1], interpolation = cv2.INTER_NEAREST)\n",
    "    lIm = cv2.resize(lIm, blurred_hIm.shape[::-1], interpolation = cv2.INTER_NEAREST)\n",
    "    \n",
    "    #Get dimensions of hIm\n",
    "    nrow, ncol = hIm.shape\n",
    "    \n",
    "    #Get posible values of (x, y) that is top left corner of patch\n",
    "    x = np.random.permutation(np.arange(0, nrow - 2 * patch_size - 1)) + patch_size\n",
    "    y = np.random.permutation(np.arange(0, ncol - 2 * patch_size - 1)) + patch_size\n",
    "    \n",
    "    #Generated Meshgrid\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    #Flatten X and Y column wise\n",
    "    xrow = X.flatten(order = 'F')\n",
    "    ycol = Y.flatten(order = 'F')\n",
    "    \n",
    "    if patch_num < len(xrow):\n",
    "        xrow = xrow[: patch_num]\n",
    "        ycol = ycol[: patch_num]\n",
    "    \n",
    "    patch_num = len(xrow)\n",
    "    \n",
    "    H = np.zeros(shape = (patch_size ** 2, patch_num))\n",
    "    L = np.zeros(shape = (4 * (patch_size ** 2), patch_num))\n",
    "    \n",
    "    #Compute first order derivatives\n",
    "    hf1 = np.array([-1,0,1]).reshape((1, -1))\n",
    "    vf1 = hf1.T\n",
    "    \n",
    "    lImG11 = signal.convolve2d(lIm, hf1[::-1, ::-1],'same') #row wise 1st order derivative\n",
    "    lImG12 = signal.convolve2d(lIm, vf1[::-1, ::-1],'same') #column wise 1st order derivative\n",
    "    \n",
    "    #Compute second order derivatives\n",
    "    hf2 = np.array([1,0,-2,0,1]).reshape((1, -1))\n",
    "    vf2 = hf2.T\n",
    "    \n",
    "    lImG21 = signal.convolve2d(lIm, hf2[::-1, ::-1], 'same') #row wise 2nd order derivative\n",
    "    lImG22 = signal.convolve2d(lIm, vf2[::-1, ::-1], 'same') #column wise 2nd order derivative\n",
    "    \n",
    "    for idx in range(patch_num):\n",
    "        row, col = xrow[idx], ycol[idx]\n",
    "        \n",
    "        #Get the patch from High Resolution Image\n",
    "        Hpatch = hIm[row: row + patch_size, col: col + patch_size].flatten(order = 'F')\n",
    "        H[:, idx] = Hpatch - np.mean(Hpatch)\n",
    "        \n",
    "        #Get the patch from Low Resolution Image\n",
    "        Lpatch1 = lImG11[row:row+patch_size,col:col+patch_size].flatten(order = 'F')\n",
    "        Lpatch2 = lImG12[row:row+patch_size,col:col+patch_size].flatten(order = 'F')\n",
    "        Lpatch3 = lImG21[row:row+patch_size,col:col+patch_size].flatten(order = 'F')\n",
    "        Lpatch4 = lImG22[row:row+patch_size,col:col+patch_size].flatten(order = 'F')\n",
    "\n",
    "        Lpatch = np.concatenate((Lpatch1, Lpatch2, Lpatch3, Lpatch4))\n",
    "        L[:, idx] = Lpatch\n",
    "    \n",
    "    return H, L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly Sample Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly Sample Patches\n",
    "#img_path: path of image\n",
    "#img_type: type of image\n",
    "#patch_size: size of patch\n",
    "#num_patch: number of patches we want to sample\n",
    "#upscale: upscale factor\n",
    "def rnd_smp_patch(img_path, img_type, patch_size, num_patch, upscale):\n",
    "    img_dir = [file for file in os.listdir(img_path) if file.endswith(img_type)]\n",
    "        \n",
    "    Xh = [] #Store High Resolution Patches\n",
    "    Xl = [] #Store Low Resolution Patches\n",
    "    \n",
    "    img_num = len(img_dir) #Total number of images\n",
    "    nper_img = np.zeros(shape = (img_num, ))\n",
    "    print(\"Number of Images From Training Dataset we have Sampled: \", img_num)\n",
    "    \n",
    "    for idx in range(img_num):\n",
    "        im = cv2.imread(os.path.join(img_path, img_dir[idx]))\n",
    "        nper_img[idx] = im.size\n",
    "    \n",
    "    nper_img = np.floor(nper_img * num_patch / np.sum(nper_img)).astype(int) #number of patches per image\n",
    "    \n",
    "    for idx in range(img_num):\n",
    "        patch_num = nper_img[idx] #number of patches from this image to select\n",
    "        im = cv2.imread(os.path.join(img_path, img_dir[idx]))\n",
    "        \n",
    "        #Sample the Patches\n",
    "        H, L = sample_patches(im, patch_size, patch_num, upscale)\n",
    "                \n",
    "        #Append to Xh and Xl\n",
    "        Xh.append(H)\n",
    "        Xl.append(L)\n",
    "    \n",
    "    Xh, Xl = np.concatenate(Xh, axis = 1), np.concatenate(Xl, axis = 1)\n",
    "    return Xh, Xl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jointly Train Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_coupled_dict(Xh, Xl, dict_size, step_size, lamb, threshold, max_iter):\n",
    "    print(\"STARTING TRAINING\")\n",
    "    N, M = Xh.shape[0], Xl.shape[0]\n",
    "    \n",
    "    a1 = 1 / np.sqrt(N)\n",
    "    a2 = 1 / np.sqrt(M)\n",
    "    \n",
    "    #Initialize Xc\n",
    "    Xc = np.concatenate((a1 * Xh, a2 * Xl), axis = 0)\n",
    "    print(f\"Xc shape: {Xc.shape}\")\n",
    "    \n",
    "    #Initialize D as a random Gaussian Matrix\n",
    "    Dc = np.random.normal(size = (N + M, dict_size))\n",
    "    Dc = normalize(Dc)\n",
    "    print(f\"Dc shape: {Dc.shape}\")\n",
    "    \n",
    "    #cap maximum iterations at max_iter\n",
    "    for iter in range(max_iter):\n",
    "        Z = lasso_optimization(Xc, Dc, lamb)\n",
    "        \n",
    "        Xc_pred = Dc @ Z\n",
    "        print(f\"Iteration {iter + 1}/{max_iter} Linear Programming Stat: {np.linalg.norm(Xc - Xc_pred) / np.linalg.norm(Xc)}\")\n",
    "        \n",
    "        Dc = quadratic_programming(Xc, Z, Dc.shape[0], Dc.shape[1], step_size, threshold, 30)\n",
    "        \n",
    "        Xc_pred = Dc @ Z\n",
    "        print(f\"Iteration {iter + 1}/{max_iter} Quadratic Programming Stat: {np.linalg.norm(Xc - Xc_pred) / np.linalg.norm(Xc)}\")\n",
    "    \n",
    "    return Dc\n",
    "\n",
    "#Lasso Optimization\n",
    "def lasso_optimization(Xc, Dc, lamb):\n",
    "    clf = linear_model.Lasso(alpha = lamb, max_iter = 100000, fit_intercept = False)\n",
    "    clf.fit(Dc, Xc)\n",
    "    return clf.coef_.T\n",
    "        \n",
    "#prox operator\n",
    "def prox(x, alpha):\n",
    "    return np.piecewise(x, [x < -alpha, (x >= -alpha) & (x <= alpha), x >= alpha], [lambda x: x + alpha, 0, lambda x: x - alpha])\n",
    "\n",
    "## Solve the Linear Programming Portion of Joint Dictionary Training\n",
    "## Goal: Find Z that minimizes || X - DZ||_2^2 + lambda * ||Z||_1\n",
    "def linear_programming(X: np.ndarray, D: np.ndarray, Zr, Zc, step_size, lamb, threshold, max_iter):\n",
    "    Z = np.random.normal(size = (Zr, Zc))\n",
    "    \n",
    "    #Run Proximal Gradient Descent\n",
    "    loss = (np.linalg.norm(X - (D @ Z)) ** 2) + (lamb * np.sum(np.abs(Z)))\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        grad = (-2 * (D.T @ X)) + (2 * (D.T @ D @ Z))\n",
    "        \n",
    "        #Update Z\n",
    "        Z = Z - (step_size * grad)\n",
    "        Z = prox(Z, step_size * lamb)\n",
    "        \n",
    "        loss = (np.linalg.norm(X - (D @ Z)) ** 2) + (lamb * np.sum(np.abs(Z)))\n",
    "        if np.linalg.norm(grad) <= threshold:\n",
    "            break\n",
    "    \n",
    "    # print(f\"Loss at Iteration {iter} = {loss}, Magnitude of Gradient = {np.linalg.norm(grad)}\")\n",
    "    return Z\n",
    "\n",
    "def normalize(D: np.ndarray):\n",
    "    norms = np.linalg.norm(D, axis=0)  # Calculate column norms\n",
    "    mask = norms > 1  # Find columns with norms > 1\n",
    "    D[:, mask] /= norms[mask]  # Normalize only columns with norms > 1\n",
    "    return D\n",
    "\n",
    "def quadratic_programming(X: np.ndarray, Z: np.ndarray, Dr, Dc, step_size, threshold, max_iter):    \n",
    "    #Run Projected Gradient Descent\n",
    "    D = np.random.normal(size = (Dr, Dc))\n",
    "    D = normalize(D)\n",
    "    loss = (np.linalg.norm(X - (D @ Z)) ** 2) / (np.linalg.norm(X))\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        grad = (-2 * (X @ Z.T)) + (2 * (D @ Z @ Z.T))\n",
    "        \n",
    "        D = D - (step_size * grad)\n",
    "        D = normalize(D)\n",
    "        loss = (np.linalg.norm(X - (D @ Z)) ** 2) / (np.linalg.norm(X))\n",
    "        \n",
    "        if np.linalg.norm(grad) <= threshold:\n",
    "            break\n",
    "    \n",
    "        # print(f\"Loss at Iteration {iter} = {loss}, Magnitude of Gradient = {np.linalg.norm(grad)}\")\n",
    "    return D\n",
    "    \n",
    "#Quadratic Programing\n",
    "def quadratic_objective_function(D_flat, X, Z):\n",
    "    # Reshape the flattened D to its original shape\n",
    "    D = D_flat.reshape(X.shape[0], -1)\n",
    "    # Compute the objective function\n",
    "    return np.linalg.norm(X - np.dot(D, Z)) ** 2\n",
    "\n",
    "def quadratic_constraints(D_flat):\n",
    "    # Reshape the flattened D to its original shape\n",
    "    D = D_flat.reshape(D_flat.shape[0], -1)\n",
    "    # Compute the norm squared for each column of D\n",
    "    norm_squared = np.sum(D**2, axis=0)\n",
    "    # Return the constraint function as a vector\n",
    "    return norm_squared - 1\n",
    "\n",
    "def quadratic_programming_scipy(X, Z):\n",
    "    # Initial guess for D\n",
    "    initial_guess_D = np.random.rand(X.shape[0] * Z.shape[0])\n",
    "\n",
    "    # Define additional arguments for the objective and constraint functions\n",
    "    args = (X, Z)\n",
    "\n",
    "    # Minimize the objective function with the constraint\n",
    "    result = minimize(quadratic_objective_function, initial_guess_D, args=args, method='BFGS', constraints={'type': 'ineq', 'fun': quadratic_constraints})\n",
    "\n",
    "    # Extract the optimized D\n",
    "    optimized_D = result.x.reshape(X.shape[0], Z.shape[0])\n",
    "    \n",
    "    return optimized_D\n",
    "\n",
    "def quadratic_programming_CVXPY(X, Z):\n",
    "    X = cp.Constant(X)  # Assuming X is an n x p numpy array\n",
    "    Z = cp.Constant(Z)  # Assuming Z is an m x p numpy array\n",
    "    \n",
    "    # Define the optimization variables\n",
    "    D = cp.Variable((X.shape[0], Z.shape[0]))  # Assuming n x m matrix D\n",
    "\n",
    "    # Define the objective function\n",
    "    objective = cp.Minimize(cp.sum_squares(X - D @ Z))\n",
    "\n",
    "    # Define the constraints using numpy operations\n",
    "    norm_squared = cp.sum(D**2, axis=0)  # Calculate the norm squared of each column of D\n",
    "    constraints = [cp.norm(D, 'fro', axis=(0, 1)) <= np.sqrt(D.shape[1])]\n",
    "\n",
    "    # Formulate the optimization problem\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "\n",
    "    # Solve the problem\n",
    "    problem.solve()\n",
    "\n",
    "    # Get the optimized D\n",
    "    optimized_D = D.value\n",
    "    \n",
    "    return optimized_D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image_path = \"../Data/Training\"\n",
    "\n",
    "dict_size = 512 #Dictionary Size will be 512\n",
    "lamb = 0.15 #sparsity regularization\n",
    "patch_size = 5 #size of patches will be 3 x 3\n",
    "nSmp = 10000 #number of patches to sample\n",
    "upscale = 4 #upscale factor\n",
    "\n",
    "#randomly generate patches\n",
    "print(\"Going to Randomly Generate Patches\")\n",
    "Xh, Xl = rnd_smp_patch(training_image_path, '.bmp', patch_size, nSmp, upscale)\n",
    "\n",
    "#Prune patches with small variance\n",
    "print(\"Going to Prune Patches\")\n",
    "Xh, Xl = patch_pruning(Xh, Xl, threshold = 1)\n",
    "\n",
    "print(Xh.shape, Xl.shape)\n",
    "\n",
    "#Joint Dictionary Training\n",
    "start_time = time.time()\n",
    "step_size = 0.0001\n",
    "threshold = 0.0001\n",
    "max_iter = 10\n",
    "print(\"Going to Jointly Train Dictionaries\")\n",
    "Dc = train_coupled_dict(Xh, Xl, dict_size, step_size, lamb, threshold, max_iter)\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken to Jointly Train Dictionaries: {elapsed_time}\")\n",
    "\n",
    "N, M = Xh.shape[0], Xl.shape[0]\n",
    "Dh, Dl = Dc[:N], Dc[N:]\n",
    "\n",
    "#Save Dh and Dl to npy files\n",
    "np.save('Dh.npy', Dh)\n",
    "np.save('Dl.npy', Dl)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
